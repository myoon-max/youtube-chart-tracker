import os
import re
import time
import json
import requests
import traceback
from datetime import datetime
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.common.by import By

# ================= [1] ÏÑ§Ï†ï =================
YOUTUBE_API_KEY = "AIzaSyDFFZNYygA85qp5p99qUG2Mh8Kl5qoLip4"

TARGET_URLS = {
    "KR_Daily_Trending": "https://charts.youtube.com/charts/TrendingVideos/kr/RightNow",
    "US_Daily_Trending": "https://charts.youtube.com/charts/TrendingVideos/us/RightNow",
    "KR_Daily_Top_MV": "https://charts.youtube.com/charts/TopVideos/kr/daily",
    "US_Daily_Top_MV": "https://charts.youtube.com/charts/TopVideos/us/daily",
    "KR_Weekly_Top_MV": "https://charts.youtube.com/charts/TopVideos/kr/weekly",
    "US_Weekly_Top_MV": "https://charts.youtube.com/charts/TopVideos/us/weekly",
    "KR_Weekly_Top_Songs": "https://charts.youtube.com/charts/TopSongs/kr/weekly",
    "US_Weekly_Top_Songs": "https://charts.youtube.com/charts/TopSongs/us/weekly",
    "KR_Daily_Top_Shorts": "https://charts.youtube.com/charts/TopShortsSongs/kr/daily",
    "US_Daily_Top_Shorts": "https://charts.youtube.com/charts/TopShortsSongs/us/daily"
}

EXTRA_URLS = {
    "Melon_Daily_Top100": "https://www.melon.com/chart/day/index.htm",
    "Genie_Daily_Top200": "https://www.genie.co.kr/chart/top200",
    "Spotify_Global_Daily": "https://kworb.net/spotify/country/global_daily.html",
    "Spotify_US_Daily": "https://kworb.net/spotify/country/us_daily.html",
    "Spotify_KR_Daily": "https://kworb.net/spotify/country/kr_daily.html",
    "Billboard_Hot100": "https://kworb.net/charts/billboard/hot100.html"
}

# ================= Ïú†Ìã∏Î¶¨Ìã∞ =================
def parse_count_strict(text):
    if not text: return 0
    t = str(text).lower().strip().replace(',', '')
    multiplier = 1
    if 'k' in t: multiplier = 1_000
    elif 'm' in t: multiplier = 1_000_000
    elif 'b' in t: multiplier = 1_000_000_000
    clean = re.sub(r'[^\d.]', '', t)
    if not clean: return 0
    try:
        val = float(clean)
        return int(val * multiplier)
    except: return 0

def clean_text(text):
    return re.sub(r'\s+', ' ', text).strip()

def get_driver():
    chrome_options = Options()
    chrome_options.add_argument("--headless=new")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument("--window-size=1920,1080")
    chrome_options.add_argument("--lang=en-US")
    chrome_options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36")
    service = Service(ChromeDriverManager().install())
    return webdriver.Chrome(service=service, options=chrome_options)

# ================= 1. YouTube Scraper (ÏõêÎ≥∏ Ïú†ÏßÄ) =================
def scrape_youtube_chart(chart_name, url, driver):
    print(f"üöÄ Scraping YouTube {chart_name}...")
    driver.get(url)
    time.sleep(5)
    
    data_list = []
    today = datetime.now().strftime("%Y-%m-%d")
    
    last_height = driver.execute_script("return document.body.scrollHeight")
    for _ in range(20):
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(0.5)
        new_height = driver.execute_script("return document.body.scrollHeight")
        if new_height == last_height: break
        last_height = new_height
    time.sleep(2)

    soup = BeautifulSoup(driver.page_source, 'html.parser')
    rows = soup.find_all('ytmc-entry-row')

    for idx, row in enumerate(rows):
        try:
            title = row.find('div', class_='title').get_text(strip=True)
            artist_tag = row.find('span', class_='artistName') or row.find('div', class_='subtitle')
            artist = artist_tag.get_text(strip=True) if artist_tag else ""
            
            row_html = str(row)
            vid = ""
            anchor = row.find('a')
            if anchor and 'href' in anchor.attrs:
                m = re.search(r"v=([A-Za-z0-9_-]{11})", anchor['href'])
                if m: vid = m.group(1)
            if not vid:
                img = row.find('img')
                if img and 'src' in img.attrs:
                    m = re.search(r'/vi(?:_webp)?/([a-zA-Z0-9_-]{11})', img['src'])
                    if m: vid = m.group(1)

            final_views = 0
            if "Trending" in chart_name: pass 
            elif "Daily_Top_MV" in chart_name:
                hidden = row.find_all('div', class_='tablet-non-displayed-metric')
                for h in hidden:
                    v = parse_count_strict(h.get_text())
                    if v > final_views: final_views = v
            elif "Weekly" in chart_name:
                metrics = row.find_all('div', class_='metric')
                if metrics: final_views = parse_count_strict(metrics[-1].get_text())

            # [Ï§ëÏöî] Í∏∞Ï°¥ ÏãúÌä∏ Ìè¨Îß∑ Ïú†ÏßÄ (Video_ID, Views)
            data_list.append({
                "Date": today, 
                "Chart": chart_name, 
                "Rank": idx+1,
                "Title": title, 
                "Artist": artist, 
                "Video_ID": vid, 
                "Views": final_views
            })
        except: continue
        
    return data_list

# ================= 2. Melon Scraper =================
def scrape_melon():
    print("üçà Scraping Melon Daily...")
    url = EXTRA_URLS["Melon_Daily_Top100"]
    headers = {'User-Agent': 'Mozilla/5.0'}
    data = []
    today = datetime.now().strftime("%Y-%m-%d")

    try:
        res = requests.get(url, headers=headers)
        soup = BeautifulSoup(res.text, 'html.parser')
        rows = soup.select('tr.lst50, tr.lst100')

        for row in rows:
            try:
                rank = int(row.select_one('span.rank').text)
                title = row.select_one('div.ellipsis.rank01 > span > a').text.strip()
                artist = row.select_one('div.ellipsis.rank02 > a').text.strip()
                
                # Í∏∞Ï°¥ Ìè¨Îß∑: Video_IDÎäî Í≥µÎûÄ, ViewsÎäî 0
                data.append({
                    "Date": today,
                    "Chart": "Melon_Daily_Top100",
                    "Rank": rank,
                    "Title": title,
                    "Artist": artist,
                    "Video_ID": "",
                    "Views": 0
                })
            except: continue
        print(f"‚úÖ Melon: {len(data)} rows")
    except Exception as e: print(f"‚ùå Melon Error: {e}")
    return data

# ================= 3. Genie Scraper =================
def scrape_genie():
    print("üßû Scraping Genie Daily...")
    headers = {'User-Agent': 'Mozilla/5.0'}
    data = []
    today = datetime.now().strftime("%Y-%m-%d")
    try:
        for page in range(1, 3):
            res = requests.get(f"{EXTRA_URLS['Genie_Daily_Top200']}?pg={page}", headers=headers)
            soup = BeautifulSoup(res.text, 'html.parser')
            rows = soup.select('tbody > tr.list')
            for row in rows:
                try:
                    rank = int(row.select_one('td.number').text.split()[0])
                    title = row.select_one('a.title').text.strip()
                    artist = row.select_one('a.artist').text.strip()
                    
                    data.append({
                        "Date": today,
                        "Chart": "Genie_Daily_Top200",
                        "Rank": rank,
                        "Title": title,
                        "Artist": artist,
                        "Video_ID": "",
                        "Views": 0
                    })
                except: continue
        print(f"‚úÖ Genie: {len(data)} rows")
    except Exception as e: print(f"‚ùå Genie Error: {e}")
    return data

# ================= 4. Kworb Scraper (Spotify/Billboard) - [FIXED] =================
def scrape_kworb(chart_key, url):
    print(f"üü¢ Scraping {chart_key} via Kworb...")
    data = []
    today = datetime.now().strftime("%Y-%m-%d")
    platform = "Billboard" if "Billboard" in chart_key else "Spotify"

    try:
        res = requests.get(url)
        res.encoding = 'utf-8'
        soup = BeautifulSoup(res.text, 'html.parser')
        rows = soup.select('tbody > tr')

        for row in rows:
            cols = row.find_all('td')
            if not cols: continue
            try:
                # Rank: Ï≤´ Î≤àÏß∏ Ïª¨Îüº
                rank_raw = cols[0].get_text(strip=True)
                if not rank_raw.isdigit(): continue
                rank = int(rank_raw)

                # [ÌïµÏã¨ ÏàòÏ†ï] Ïª¨Îüº Î∞ÄÎ¶º Î∞©ÏßÄ Î°úÏßÅ
                # cols[1]Ïù¥ "+1", "-5", "=" Í∞ôÏùÄ Ï∂îÏù¥ Ï†ïÎ≥¥Î©¥ -> cols[2]Í∞Ä Í∞ÄÏàò/Ï†úÎ™©
                # cols[1]Ïù¥ Í∞ÄÏàò/Ï†úÎ™©Ïù¥Î©¥ -> Í∑∏ÎåÄÎ°ú ÏÇ¨Ïö©
                col_idx = 1
                check_text = cols[1].get_text(strip=True)
                if re.match(r'^[\+\-=]?\d*|NEW|RE$', check_text): 
                    col_idx = 2 # Ï∂îÏù¥ Ïª¨ÎüºÏù¥ ÏûàÏúºÎØÄÎ°ú Ìïú Ïπ∏ Îí§Î°ú Ïù¥Îèô

                # Artist - Title Î∂ÑÎ¶¨
                full_text = clean_text(cols[col_idx].get_text())
                if " - " in full_text:
                    parts = full_text.split(" - ", 1)
                    artist = parts[0].strip()
                    title = parts[1].strip()
                else:
                    artist = "Unknown" # ÏùºÎã® Unknown Î∞ïÍ≥†
                    title = full_text  # ÌÜµÏúºÎ°ú ÎÑ£Ïùå

                # Streams (SpotifyÎßå Ï°¥Ïû¨, Î≥¥ÌÜµ Îß® Îí§ÏóêÏÑú ÎëêÎ≤àÏß∏ ÌòπÏùÄ ÌäπÏ†ï ÏúÑÏπò)
                streams = 0
                if platform == "Spotify":
                    # Kworb ÌÖåÏù¥Î∏î Íµ¨Ï°∞ÏÉÅ StreamsÎäî Î≥¥ÌÜµ 'Total' Ïïû Ïª¨Îüº
                    # ÏïàÏ†ÑÌïòÍ≤å Îí§ÏóêÏÑúÎ∂ÄÌÑ∞ Ïà´Ïûê ÌòïÏãù Ï∞æÍ∏∞
                    for c in reversed(cols):
                        txt = c.get_text().replace(',', '').strip()
                        if txt.isdigit() and len(txt) > 3: # 1000 Ïù¥ÏÉÅ Ïà´ÏûêÎ©¥ Ïä§Ìä∏Î¶¨Î∞çÏúºÎ°ú Í∞ÑÏ£º
                            streams = int(txt)
                            break

                data.append({
                    "Date": today,
                    "Chart": chart_key,
                    "Rank": rank,
                    "Title": title,
                    "Artist": artist,
                    "Video_ID": "", # Spotify/BillboardÎäî VideoID ÏóÜÏùå
                    "Views": streams
                })
            except: continue
        print(f"‚úÖ {chart_key}: {len(data)} rows")
    except Exception as e: print(f"‚ùå Kworb Error ({chart_key}): {e}")
    return data

# ================= API Î≥¥Ï†ï (Ïú†ÌäúÎ∏å Trending) =================
def fill_youtube_api_stats(data_list):
    ids = [d["Video_ID"] for d in data_list if d["Video_ID"] and "Trending" in d["Chart"]]
    if not ids: return
    
    print(f"üîç Fetching API stats for {len(ids)} videos...")
    url = "https://www.googleapis.com/youtube/v3/videos"
    for i in range(0, len(ids), 50):
        chunk = ids[i:i+50]
        params = {"part": "statistics", "id": ",".join(chunk), "key": YOUTUBE_API_KEY}
        try:
            res = requests.get(url, params=params).json()
            if "items" in res:
                stats_map = {item["id"]: int(item["statistics"].get("viewCount", 0)) for item in res["items"]}
                for item in data_list:
                    if item["Video_ID"] in stats_map:
                        item["Views"] = stats_map[item["Video_ID"]]
        except: pass

# ================= Î©îÏù∏ Ïã§Ìñâ =================
if __name__ == "__main__":
    all_data = []
    driver = None

    try:
        # 1. YouTube
        try:
            driver = get_driver()
            for name, url in TARGET_URLS.items():
                try:
                    yt_data = scrape_youtube_chart(name, url, driver)
                    if "Trending" in name:
                        fill_youtube_api_stats(yt_data)
                    all_data.extend(yt_data)
                    print(f"‚úÖ YouTube {name}: {len(yt_data)} rows.")
                except Exception as e: print(f"‚ö†Ô∏è Error on YouTube {name}: {e}")
        except Exception as yt_e: print(f"üî• YouTube Driver Error: {yt_e}")
        finally:
            if driver: driver.quit()

        # 2. Others
        all_data.extend(scrape_melon())
        all_data.extend(scrape_genie())
        for key, url in EXTRA_URLS.items():
            if "Spotify" in key or "Billboard" in key:
                all_data.extend(scrape_kworb(key, url))

        # 3. Send
        webhook = os.environ.get("APPS_SCRIPT_WEBHOOK")
        if all_data and webhook:
            print(f"üöÄ Sending {len(all_data)} rows...")
            chunk_size = 2000
            for i in range(0, len(all_data), chunk_size):
                chunk = all_data[i:i+chunk_size]
                requests.post(webhook, json=chunk)
                print(f"  -> Chunk {i//chunk_size + 1} sent.")
                time.sleep(1)
            print("‚ú® All Done!")
        else:
            print("‚ö†Ô∏è No webhook or data.")

    except Exception as main_e:
        print("üî• FATAL ERROR: Script crashed.")
        print(traceback.format_exc())
