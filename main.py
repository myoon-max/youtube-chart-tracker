import os
import json
import requests
import pandas as pd
from datetime import datetime
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from bs4 import BeautifulSoup
import time
import re

# ================= ì„¤ì • =================
YOUTUBE_API_KEY = "AIzaSyDFFZNYygA85qp5p99qUG2Mh8Kl5qoLip4"

TARGET_URLS = {
    "KR_Daily_Trending": "https://charts.youtube.com/charts/TrendingVideos/kr/RightNow",
    "KR_Daily_Top_MV": "https://charts.youtube.com/charts/TopVideos/kr/daily",
    "KR_Weekly_Top_MV": "https://charts.youtube.com/charts/TopVideos/kr/weekly",
    "KR_Weekly_Top_Songs": "https://charts.youtube.com/charts/TopSongs/kr/weekly",
    "US_Daily_Trending": "https://charts.youtube.com/charts/TrendingVideos/us/RightNow",
    "US_Daily_Top_MV": "https://charts.youtube.com/charts/TopVideos/us/daily",
    "US_Weekly_Top_MV": "https://charts.youtube.com/charts/TopVideos/us/weekly",
    "US_Weekly_Top_Songs": "https://charts.youtube.com/charts/TopSongs/us/weekly",
    # ì‡¼ì¸  ì°¨íŠ¸ (ì´ì œ ì—¬ê¸°ì„œëŠ” ì¡°íšŒìˆ˜ ë§ê³  'ê°œìˆ˜'ë¥¼ ìº¡ë‹ˆë‹¤)
    "KR_Daily_Top_Shorts": "https://charts.youtube.com/charts/TopShortsSongs/kr/daily",
    "US_Daily_Top_Shorts": "https://charts.youtube.com/charts/TopShortsSongs/us/daily"
}

# ================= ìˆ«ì ë³€í™˜ê¸° =================
def parse_count(text):
    if not text: return 0
    text = str(text).lower().replace('shorts', '').replace('videos', '').replace('ì¡°íšŒìˆ˜', '').strip()
    try:
        multiplier = 1
        if 'm' in text: multiplier = 1_000_000
        elif 'k' in text: multiplier = 1_000
        elif 'b' in text: multiplier = 1_000_000_000
        
        # ìˆ«ìì™€ ì (.)ë§Œ ë‚¨ê¸°ê³  ë³€í™˜ (ì½¤ë§ˆ ì œê±°)
        clean_text = re.sub(r'[^\d.]', '', text)
        if not clean_text: return 0
        
        return int(float(clean_text) * multiplier)
    except: return 0

# ================= [í•µì‹¬] ì‡¼ì¸  ìƒì„± ê°œìˆ˜ í¬ë¡¤ë§ (Deep Dive) =================
def get_shorts_creation_count(driver, video_id):
    if not video_id: return 0
    
    # ì§€ë¦„ê¸¸ URL (Source Page)
    target_url = f"https://www.youtube.com/source/{video_id}/shorts"
    
    try:
        driver.get(target_url)
        # ë¡œë”© ëŒ€ê¸° (ë¹ ë¥´ê²Œ í›‘ê¸° ìœ„í•´ ì§§ê²Œ)
        time.sleep(2) 
        
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        
        # íŒ¨í„´: "82K shorts", "1.5M videos", "321 shorts" ë“±ì„ ì°¾ìŒ
        # ë³´í†µ í—¤ë”ë‚˜ ë©”íƒ€ë°ì´í„° ìª½ì— ìˆìŒ. ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ íŒ¨í„´ ê²€ìƒ‰ì´ ê°€ì¥ í™•ì‹¤í•¨.
        body_text = soup.get_text(separator=' ', strip=True)
        
        # íŒ¨í„´ 1: "82K shorts" í˜•íƒœ
        match = re.search(r'([\d,.]+[M|K|B]?)\s*shorts', body_text, re.IGNORECASE)
        if match:
            return parse_count(match.group(1))
            
        # íŒ¨í„´ 2: "123 videos" í˜•íƒœ (ê°€ë” ì´ë ‡ê²Œ ëœ¸)
        match2 = re.search(r'([\d,.]+[M|K|B]?)\s*videos', body_text, re.IGNORECASE)
        if match2:
            return parse_count(match2.group(1))

        return 0
    except:
        return 0

# ================= í¬ë¡¤ë§ ë¡œì§ =================
def get_driver():
    chrome_options = Options()
    chrome_options.add_argument("--headless") 
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--window-size=1920,1080")
    chrome_options.add_argument("--lang=en-US") 
    service = Service(ChromeDriverManager().install())
    return webdriver.Chrome(service=service, options=chrome_options)

def scrape_chart(driver, chart_name, url):
    print(f"ğŸš€ Scraping {chart_name}...")
    driver.get(url)
    time.sleep(10)
    driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
    time.sleep(5)
    
    soup = BeautifulSoup(driver.page_source, 'html.parser')
    rows = soup.find_all('ytmc-entry-row')
    
    # ì¬ì‹œë„
    if not rows:
        time.sleep(5)
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        rows = soup.find_all('ytmc-entry-row')
    
    data = []
    today = datetime.now().strftime("%Y-%m-%d")
    rank = 1
    
    # ì‡¼ì¸  ì°¨íŠ¸ì¸ì§€ í™•ì¸ (ì‡¼ì¸  ì°¨íŠ¸ë©´ 'ê°œìˆ˜' ìºëŸ¬ ê°€ì•¼ í•¨)
    is_shorts_chart = "Shorts" in chart_name
    
    for row in rows:
        try:
            title = row.find('div', class_='title').get_text(strip=True)
            
            artist_tag = row.find('span', class_='artistName')
            if not artist_tag: artist_tag = row.find('div', class_='subtitle')
            artist = artist_tag.get_text(strip=True) if artist_tag else ""
            
            # Video ID ì¶”ì¶œ (ì¸ë„¤ì¼ì—ì„œ)
            img = row.find('img')
            vid = ""
            if img and 'src' in img.attrs and '/vi/' in img['src']:
                vid = img['src'].split('/vi/')[1].split('/')[0]
            
            final_value = 0
            
            # ==========================================
            # CASE 1: ì‡¼ì¸  ì°¨íŠ¸ë‹¤? -> Source Pageë¡œ ì ì…
            # ==========================================
            if is_shorts_chart and vid:
                # ì—¬ê¸°ì„œ ë°”ë¡œ ì´ë™í•˜ë©´ ë£¨í”„ê°€ ê¹¨ì§€ë‹ˆê¹Œ, ì¼ë‹¨ IDë§Œ ì €ì¥í•˜ê³  ë‚˜ì¤‘ì— í•œêº¼ë²ˆì— ë”
                # í•˜ì§€ë§Œ ì½”ë“œ ë‹¨ìˆœí™”ë¥¼ ìœ„í•´ ì¼ë‹¨ ì—¬ê¸°ì„œ ì €ì¥í•´ë‘ê³  ë©”ì¸ ë£¨í”„ì—ì„œ ì²˜ë¦¬ ê¶Œì¥.
                # ì—¬ê¸°ì„œëŠ” 0ìœ¼ë¡œ ë‘ê³ , vidë¥¼ í™•ì‹¤íˆ ì±™ê¹€.
                pass 

            # ==========================================
            # CASE 2: ì¼ë°˜/Top Songs ì°¨íŠ¸ë‹¤? -> ì½¤ë§ˆ ìˆ«ì ì‚¬ëƒ¥
            # ==========================================
            else:
                all_divs = row.find_all('div')
                views_text = ""
                
                # ë’¤ì—ì„œë¶€í„° í›‘ìœ¼ë©´ì„œ "4,842,974" ê°™ì€ íŒ¨í„´ ì°¾ê¸°
                for div in reversed(all_divs):
                    txt = div.get_text(strip=True)
                    # ì½¤ë§ˆê°€ í¬í•¨ëœ ìˆ«ì (ì˜ˆ: 1,234 / 1,234,567)
                    # ë­í¬(1~100)ë‚˜ ì£¼ê°„(1~500)ê³¼ êµ¬ë¶„í•˜ê¸° ìœ„í•´ 'ì½¤ë§ˆ' í•„ìˆ˜ ì¡°ê±´
                    if re.match(r'^\d{1,3}(,\d{3})+$', txt):
                        views_text = txt
                        break
                    # í˜¹ì‹œ 1.5M ê°™ì€ê±°ì¼ìˆ˜ë„ ìˆìœ¼ë‹ˆ
                    if re.search(r'\d+(\.\d+)?[MKB]', txt, re.IGNORECASE):
                        views_text = txt
                        # ìš°ì„ ìˆœìœ„ ë‚®ìŒ (break ì•ˆí•¨)
                
                final_value = parse_count(views_text)

            data.append({
                "Date": today,
                "Chart": chart_name,
                "Rank": rank,
                "Title": title,
                "Artist": artist,
                "Video_ID": vid,
                "Views": final_value # ì‡¼ì¸ ëŠ” ë‚˜ì¤‘ì— ì—…ë°ì´íŠ¸
            })
            rank += 1
        except: continue
        
    return data

# ================= ë©”ì¸ ì‹¤í–‰ =================
if __name__ == "__main__":
    driver = get_driver()
    final_data = []
    
    for name, url in TARGET_URLS.items():
        try:
            # 1. 1ì°¨ ìˆ˜ì§‘ (ëª©ë¡ í™•ë³´)
            chart_data = scrape_chart(driver, name, url)
            
            # 2. [ì‹¬í™”] ì‡¼ì¸  ì°¨íŠ¸ë©´ -> ê° IDë§ˆë‹¤ ì†ŒìŠ¤ í˜ì´ì§€ ë°©ë¬¸ (Sê¸‰ ë¯¸ì…˜ ìˆ˜í–‰)
            if "Shorts" in name:
                print(f"  â†³ ğŸ•µï¸â€â™‚ï¸ Entering Deep Dive for {len(chart_data)} Shorts...")
                for item in chart_data:
                    if item["Video_ID"]:
                        # ê° ì˜ìƒë§ˆë‹¤ í˜ì´ì§€ ì´ë™ (ì‹œê°„ ì¢€ ê±¸ë¦¼)
                        count = get_shorts_creation_count(driver, item["Video_ID"])
                        item["Views"] = count # ì¡°íšŒìˆ˜ ëŒ€ì‹  'ê°œìˆ˜' ì €ì¥
                        # print(f"    - {item['Title']}: {count} Shorts") # ë¡œê·¸ ë„ˆë¬´ ë§ìœ¼ë©´ ì£¼ì„
            
            final_data.extend(chart_data)
            print(f"âœ… {name}: {len(chart_data)} rows done.")
            
        except Exception as e:
            print(f"Error on {name}: {e}")
            
    driver.quit()
    
    webhook = os.environ.get("APPS_SCRIPT_WEBHOOK")
    if final_data and webhook:
        print(f"Total {len(final_data)} rows. Sending...")
        try:
            requests.post(webhook, json=final_data)
            print("Success!")
        except Exception as e:
            print(f"Send Error: {e}")
