import os
import re
import time
import json
import requests
import traceback
from datetime import datetime
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

# ==========================================
# [PART 1] ì„¤ì • ë° URL ì •ì˜
# ==========================================

YOUTUBE_API_KEY = "AIzaSyDFFZNYygA85qp5p99qUG2Mh8Kl5qoLip4"

# 1. ìœ íŠœë¸Œ ì°¨íŠ¸ (Selenium)
TARGET_URLS = {
    "KR_Daily_Trending": "https://charts.youtube.com/charts/TrendingVideos/kr/RightNow",
    "US_Daily_Trending": "https://charts.youtube.com/charts/TrendingVideos/us/RightNow",
    "KR_Daily_Top_MV": "https://charts.youtube.com/charts/TopVideos/kr/daily",
    "US_Daily_Top_MV": "https://charts.youtube.com/charts/TopVideos/us/daily",
    "KR_Weekly_Top_MV": "https://charts.youtube.com/charts/TopVideos/kr/weekly",
    "US_Weekly_Top_MV": "https://charts.youtube.com/charts/TopVideos/us/weekly",
    "KR_Weekly_Top_Songs": "https://charts.youtube.com/charts/TopSongs/kr/weekly",
    "US_Weekly_Top_Songs": "https://charts.youtube.com/charts/TopSongs/us/weekly",
    "KR_Daily_Top_Shorts": "https://charts.youtube.com/charts/TopShortsSongs/kr/daily",
    "US_Daily_Top_Shorts": "https://charts.youtube.com/charts/TopShortsSongs/us/daily"
}

# 2. ë¹Œë³´ë“œ ì°¨íŠ¸ (Selenium - Official)
BILLBOARD_URLS = {
    "Billboard_Hot100": "https://www.billboard.com/charts/hot-100/",
    "Billboard_200": "https://www.billboard.com/charts/billboard-200/",
    "Billboard_Global200": "https://www.billboard.com/charts/billboard-global-200/"
}

# 3. ê¸°íƒ€ í”Œë«í¼ (Requests & Kworb)
EXTRA_URLS = {
    "Melon_Daily_Top100": "https://www.melon.com/chart/day/index.htm",
    "Genie_Daily_Top200": "https://www.genie.co.kr/chart/top200",
    "Spotify_Global_Daily": "https://kworb.net/spotify/country/global_daily.html",
    "Spotify_US_Daily": "https://kworb.net/spotify/country/us_daily.html",
    "Spotify_KR_Daily": "https://kworb.net/spotify/country/kr_daily.html"
}

# ================= ìœ í‹¸ë¦¬í‹° =================
def parse_count_strict(text):
    if not text: return 0
    t = str(text).lower().strip().replace(',', '')
    multiplier = 1
    if 'k' in t: multiplier = 1_000
    elif 'm' in t: multiplier = 1_000_000
    elif 'b' in t: multiplier = 1_000_000_000
    clean = re.sub(r'[^\d.]', '', t)
    if not clean: return 0
    try:
        val = float(clean)
        return int(val * multiplier)
    except: return 0

def clean_text(text):
    return re.sub(r'\s+', ' ', text).strip()

def get_driver():
    chrome_options = Options()
    chrome_options.add_argument("--headless=new") # í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ ìœ ì§€
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument("--window-size=1920,1080")
    chrome_options.add_argument("--lang=en-US")
    # ì°¨ë‹¨ ë°©ì§€ë¥¼ ìœ„í•œ User-Agent ì„¤ì •
    chrome_options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36")
    
    service = Service(ChromeDriverManager().install())
    return webdriver.Chrome(service=service, options=chrome_options)

# ================= Shorts & API (ìœ íŠœë¸Œìš©) =================
def get_shorts_creation_count(driver, video_id):
    if not video_id: return 0
    url = f"https://www.youtube.com/source/{video_id}/shorts"
    try:
        driver.get(url)
        time.sleep(1)
        body_text = driver.find_element(By.TAG_NAME, "body").text
        match = re.search(r'([\d,.]+[KMB]?)\s*Shorts', body_text, re.IGNORECASE)
        if match:
            return parse_count_strict(match.group(1))
        return 0
    except: return 0

def get_views_from_api(video_ids):
    if not video_ids: return {}
    url = "https://www.googleapis.com/youtube/v3/videos"
    stats_map = {}
    for i in range(0, len(video_ids), 50):
        chunk = video_ids[i:i+50]
        params = {"part": "statistics", "id": ",".join(chunk), "key": YOUTUBE_API_KEY}
        try:
            res = requests.get(url, params=params).json()
            if "items" in res:
                for item in res["items"]:
                    vid = item["id"]
                    view_count = int(item["statistics"].get("viewCount", 0))
                    stats_map[vid] = view_count
        except: pass
    return stats_map

# ================= [PART 2] í¬ë¡¤ëŸ¬ í•¨ìˆ˜ ëª¨ìŒ =================

# 1. ìœ íŠœë¸Œ ì°¨íŠ¸ í¬ë¡¤ëŸ¬
def scrape_youtube_chart(chart_name, url, driver):
    print(f"ğŸš€ Scraping YouTube: {chart_name}...")
    driver.get(url)
    time.sleep(5)
    
    data_list = []
    today = datetime.now().strftime("%Y-%m-%d")
    
    is_trending = "Trending" in chart_name
    is_shorts = "Shorts" in chart_name
    is_daily_mv = "Daily_Top_MV" in chart_name
    is_weekly = "Weekly" in chart_name
    
    # Shorts ë¡œì§
    if is_shorts:
        last_height = driver.execute_script("return document.body.scrollHeight")
        for _ in range(30):
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(0.5)
            new_height = driver.execute_script("return document.body.scrollHeight")
            if new_height == last_height: break
            last_height = new_height
        time.sleep(2)

        soup = BeautifulSoup(driver.page_source, 'html.parser')
        rows = soup.find_all('ytmc-entry-row')
        for idx, row in enumerate(rows):
            try:
                title = row.find('div', class_='title').get_text(strip=True)
                artist_tag = row.find('span', class_='artistName') or row.find('div', class_='subtitle')
                artist = artist_tag.get_text(strip=True) if artist_tag else ""
                
                row_html = str(row)
                vid = ""
                match = re.search(r'watch\?v=([a-zA-Z0-9_-]{11})', row_html)
                if match: vid = match.group(1)
                
                shorts_count = 0
                if vid: shorts_count = get_shorts_creation_count(driver, vid)
                
                data_list.append({
                    "Date": today, "Chart": chart_name, "Rank": idx+1,
                    "Title": title, "Artist": artist, "Video_ID": vid, "Views": shorts_count
                })
            except: continue

    # MV/Songs ë¡œì§
    else:
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(3)
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        rows = soup.find_all('ytmc-entry-row')
        
        for idx, row in enumerate(rows):
            try:
                title = row.find('div', class_='title').get_text(strip=True)
                artist_tag = row.find('span', class_='artistName') or row.find('div', class_='subtitle')
                artist = artist_tag.get_text(strip=True) if artist_tag else ""
                
                vid = ""
                anchor = row.find('a')
                if anchor and 'href' in anchor.attrs:
                    m = re.search(r"v=([A-Za-z0-9_-]{11})", anchor['href'])
                    if m: vid = m.group(1)
                if not vid:
                    img = row.find('img')
                    if img and 'src' in img.attrs:
                        m = re.search(r'/vi(?:_webp)?/([a-zA-Z0-9_-]{11})', img['src'])
                        if m: vid = m.group(1)

                final_views = 0
                if is_trending:
                    pass 
                elif is_daily_mv:
                    hidden_divs = row.find_all('div', class_='tablet-non-displayed-metric')
                    max_val = 0
                    for h in hidden_divs:
                        val = parse_count_strict(h.get_text(strip=True))
                        if val > max_val: max_val = val
                    final_views = max_val
                elif is_weekly:
                    metrics = row.find_all('div', class_='metric')
                    if metrics:
                        final_views = parse_count_strict(metrics[-1].get_text(strip=True))
                
                data_list.append({
                    "Date": today, "Chart": chart_name, "Rank": idx+1,
                    "Title": title, "Artist": artist, "Video_ID": vid, "Views": final_views
                })
            except: continue
            
    return data_list

# 2. [ìˆ˜ì •ë¨] ë¹Œë³´ë“œ 3ì¢… í†µí•© í¬ë¡¤ëŸ¬ (Selenium + ìŠ¤í¬ë¦°ìƒ· ê¸°ë°˜ í´ë˜ìŠ¤ ìˆ˜ì •)
def scrape_billboard_official(driver, chart_key, url):
    print(f"ğŸ‡ºğŸ‡¸ Scraping {chart_key} (Official/Selenium)...")
    data = []
    today = datetime.now().strftime("%Y-%m-%d")

    try:
        driver.get(url)
        # [ì¤‘ìš”] ìŠ¤í¬ë¡¤ ë¡œì§ ì¶”ê°€ (ë¹Œë³´ë“œëŠ” ìŠ¤í¬ë¡¤ ì•ˆí•˜ë©´ ë°ì´í„° ë¡œë”© ì•ˆë¨)
        last_height = driver.execute_script("return document.body.scrollHeight")
        # ì¡°ê¸ˆì”© ë‚´ë¦¬ë©´ì„œ ë¡œë”© ìœ ë„
        for i in range(1, 5):
            driver.execute_script(f"window.scrollTo(0, {i * 1000});")
            time.sleep(1)
        
        # ë§ˆì§€ë§‰ìœ¼ë¡œ ë§¨ ì•„ë˜ë¡œ
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(3) 

        # ì°¨íŠ¸ í–‰ ì»¨í…Œì´ë„ˆ ëŒ€ê¸°
        try:
            wait = WebDriverWait(driver, 15)
            wait.until(EC.presence_of_element_located((By.CLASS_NAME, "o-chart-results-list-row-container")))
        except:
            print(f"âš ï¸ {chart_key}: Timeout or Page Blocked.")
            return []

        soup = BeautifulSoup(driver.page_source, 'html.parser')
        
        # [ìˆ˜ì •] ìŠ¤í¬ë¦°ìƒ· 1ë²ˆì˜ ì»¨í…Œì´ë„ˆ í´ë˜ìŠ¤ ì‚¬ìš©
        rows = soup.select('div.o-chart-results-list-row-container')
        
        print(f"   -> Found {len(rows)} raw rows.")

        for idx, row in enumerate(rows):
            try:
                # 1. ìˆœìœ„ (Rank)
                # ìŠ¤í¬ë¦°ìƒ· 1ë²ˆ: 1ë“± '2'ëŠ” span.c-label.a-font-primary-bold-l ì•ˆì— ìˆìŒ
                rank_elem = row.select_one('span.c-label.a-font-primary-bold-l')
                if rank_elem:
                    rank_text = rank_elem.get_text(strip=True)
                    rank = int(rank_text) if rank_text.isdigit() else (idx + 1)
                else:
                    rank = idx + 1
                
                # 2. ì œëª© (Title) 
                # ìŠ¤í¬ë¦°ìƒ· 4ë²ˆ: h3 íƒœê·¸ì— 'c-title' í´ë˜ìŠ¤ í™•ì¸ë¨ (ID ëŒ€ì‹  í´ë˜ìŠ¤ ì‚¬ìš©)
                title_tag = row.select_one('h3.c-title')
                title = title_tag.get_text(strip=True) if title_tag else "Unknown"
                
                # 3. ê°€ìˆ˜ (Artist)
                # ìŠ¤í¬ë¦°ìƒ· 4ë²ˆ: h3 ë°”ë¡œ ì•„ë˜ span.c-label.a-no-trucate ê°€ ì•„í‹°ìŠ¤íŠ¸ì„
                artist = "Unknown"
                if title_tag:
                    # h3 ë°”ë¡œ ë‹¤ìŒì— ì˜¤ëŠ” í˜•ì œ ìš”ì†Œ ì°¾ê¸° (í˜¹ì€ ê°™ì€ li ì•ˆì˜ span ì°¾ê¸°)
                    # êµ¬ì¡°ìƒ: li > h3 ... span
                    # title_tagì˜ ë¶€ëª¨ lië¥¼ ì°¾ê³  ê·¸ ì•ˆì—ì„œ c-label.a-no-trucate ì°¾ê¸°
                    parent_li = title_tag.find_parent('li')
                    if parent_li:
                        # a-no-trucate í´ë˜ìŠ¤ê°€ ì•„í‹°ìŠ¤íŠ¸ëª…ì— ë¶™ì–´ìˆìŒ (ìŠ¤í¬ë¦°ìƒ· 4)
                        artist_span = parent_li.select_one('span.c-label.a-no-trucate')
                        if artist_span:
                            artist = artist_span.get_text(strip=True)

                data.append({
                    "Date": today, "Chart": chart_key, "Rank": rank,
                    "Title": title, "Artist": artist, "Video_ID": "", "Views": 0
                })
            except: continue
            
        print(f"âœ… {chart_key}: {len(data)} rows captured.")
    except Exception as e:
        print(f"âŒ Billboard Error ({chart_key}): {e}")
    return data

# 3. ë©œë¡  í¬ë¡¤ëŸ¬ (Requests)
def scrape_melon():
    print("ğŸˆ Scraping Melon Daily...")
    url = EXTRA_URLS["Melon_Daily_Top100"]
    headers = {'User-Agent': 'Mozilla/5.0'}
    data = []
    today = datetime.now().strftime("%Y-%m-%d")

    try:
        res = requests.get(url, headers=headers)
        soup = BeautifulSoup(res.text, 'html.parser')
        rows = soup.select('tr.lst50, tr.lst100')
        for row in rows:
            try:
                rank = int(row.select_one('span.rank').text)
                title = row.select_one('div.ellipsis.rank01 > span > a').text.strip()
                artist = row.select_one('div.ellipsis.rank02 > a').text.strip()
                data.append({
                    "Date": today, "Chart": "Melon_Daily_Top100", "Rank": rank,
                    "Title": title, "Artist": artist, "Video_ID": "", "Views": 0
                })
            except: continue
        print(f"âœ… Melon: {len(data)} rows")
    except Exception as e: print(f"âŒ Melon Error: {e}")
    return data

# 4. ì§€ë‹ˆ í¬ë¡¤ëŸ¬ (Requests)
def scrape_genie():
    print("ğŸ§ Scraping Genie Daily...")
    headers = {'User-Agent': 'Mozilla/5.0'}
    data = []
    today = datetime.now().strftime("%Y-%m-%d")
    try:
        for page in range(1, 3):
            res = requests.get(f"{EXTRA_URLS['Genie_Daily_Top200']}?pg={page}", headers=headers)
            soup = BeautifulSoup(res.text, 'html.parser')
            rows = soup.select('tbody > tr.list')
            for row in rows:
                try:
                    rank = int(row.select_one('td.number').text.split()[0])
                    title = row.select_one('a.title').text.strip()
                    artist = row.select_one('a.artist').text.strip()
                    data.append({
                        "Date": today, "Chart": "Genie_Daily_Top200", "Rank": rank,
                        "Title": title, "Artist": artist, "Video_ID": "", "Views": 0
                    })
                except: continue
        print(f"âœ… Genie: {len(data)} rows")
    except Exception as e: print(f"âŒ Genie Error: {e}")
    return data

# 5. [ìˆ˜ì •ë¨] Kworb Spotify í¬ë¡¤ëŸ¬ (ì‹¤ì œ ë‚ ì§œ íŒŒì‹± ì ìš©)
def scrape_kworb(chart_key, url):
    print(f"ğŸŸ¢ Scraping {chart_key} via Kworb (Parsing real date)...")
    data = []
    
    # ê¸°ë³¸ê°’ì€ ì˜¤ëŠ˜ (íŒŒì‹± ì‹¤íŒ¨ ëŒ€ë¹„)
    chart_date = datetime.now().strftime("%Y-%m-%d")
    TARGET_HEADER_KEYWORD = "Streams"

    try:
        res = requests.get(url)
        res.encoding = 'utf-8'
        soup = BeautifulSoup(res.text, 'html.parser')
        
        # [í•µì‹¬ ìˆ˜ì •] í˜ì´ì§€ ìƒë‹¨ì˜ ë‚ ì§œ(2025/12/02) íŒŒì‹±
        # ìŠ¤í¬ë¦°ìƒ·ì— ìˆë˜ <span class="pagetitle"> íƒ€ê²ŸíŒ…
        title_span = soup.select_one('span.pagetitle')
        if title_span:
            title_text = title_span.get_text()
            # YYYY/MM/DD í˜•ì‹ ì¶”ì¶œ
            date_match = re.search(r'(\d{4})/(\d{2})/(\d{2})', title_text)
            if date_match:
                chart_date = f"{date_match.group(1)}-{date_match.group(2)}-{date_match.group(3)}"
                print(f"   -> Detected Chart Date: {chart_date}")
            else:
                print(f"   -> âš ï¸ Date parsing failed, using today: {chart_date}")

        table = soup.find('table')
        if not table: return []

        headers = []
        thead = table.find('thead')
        if thead:
            headers = [th.get_text(strip=True) for th in thead.find_all('th')]
        else:
            first_row = table.find('tr')
            headers = [td.get_text(strip=True) for td in first_row.find_all(['td', 'th'])]

        target_idx = -1
        title_idx = -1
        for i, h in enumerate(headers):
            if "Artist" in h or "Title" in h: title_idx = i
            if TARGET_HEADER_KEYWORD in h and "+" not in h:
                target_idx = i
                break
        
        if target_idx == -1: 
            target_idx = 6 
            if title_idx == -1: title_idx = 2

        rows = soup.select('tbody > tr')
        for row in rows:
            cols = row.find_all('td')
            if not cols: continue
            try:
                rank_txt = cols[0].get_text(strip=True)
                if not rank_txt.isdigit(): continue
                rank = int(rank_txt)

                full_text = clean_text(cols[title_idx].get_text())
                if " - " in full_text:
                    parts = full_text.split(" - ", 1)
                    artist = parts[0].strip()
                    title = parts[1].strip()
                else:
                    artist = "Unknown"
                    title = full_text

                val_raw = cols[target_idx].get_text(strip=True)
                val_clean = re.sub(r'[^\d]', '', val_raw)
                final_val = int(val_clean) if val_clean else 0

                data.append({
                    "Date": chart_date, # [ìˆ˜ì •] ì‹¤ì œ íŒŒì‹±ëœ ë‚ ì§œ ì‚¬ìš©
                    "Chart": chart_key,
                    "Rank": rank,
                    "Title": title,
                    "Artist": artist,
                    "Video_ID": "",
                    "Views": final_val
                })
            except: continue
        print(f"âœ… {chart_key}: {len(data)} rows (Date: {chart_date})")
    except Exception as e: print(f"âŒ Kworb Error ({chart_key}): {e}")
    return data

# ==========================================
# [PART 3] ë©”ì¸ ì‹¤í–‰ (Main Execution)
# ==========================================
if __name__ == "__main__":
    driver = None
    final_data = [] 

    try:
        print("=== [Start] MusicDeal Crawler ===")
        
        # 1. Selenium ê¸°ë°˜ í¬ë¡¤ë§ (YouTube + Billboard)
        try:
            driver = get_driver()
            
            # (1) YouTube Scraping
            for name, url in TARGET_URLS.items():
                try:
                    chart_data = scrape_youtube_chart(name, url, driver)
                    if "Trending" in name:
                        ids = [d["Video_ID"] for d in chart_data if d["Video_ID"]]
                        if ids:
                            api_stats = get_views_from_api(ids)
                            for item in chart_data:
                                if item["Video_ID"] in api_stats:
                                    item["Views"] = api_stats[item["Video_ID"]]
                    final_data.extend(chart_data)
                except Exception as e:
                    print(f"âš ï¸ Error on YouTube {name}: {e}")
            
            # (2) Billboard Scraping (Official)
            print("\n>>> Starting Billboard Charts...")
            for b_name, b_url in BILLBOARD_URLS.items():
                final_data.extend(scrape_billboard_official(driver, b_name, b_url))

        except Exception as sel_e:
            print(f"ğŸ”¥ Selenium Process Error: {sel_e}")
            print(traceback.format_exc())
        finally:
            if driver: driver.quit()

        # 2. Requests ê¸°ë°˜ í¬ë¡¤ë§ (Melon, Genie, Spotify/Kworb)
        print("\n=== [Domestic & Spotify Charts] ===")
        final_data.extend(scrape_melon())
        final_data.extend(scrape_genie())
        
        for key, url in EXTRA_URLS.items():
            final_data.extend(scrape_kworb(key, url))

        # 3. ë°ì´í„° ì „ì†¡
        print(f"\n=== [Sending Data] Total {len(final_data)} rows ===")
        # Apps Script ì›¹í›… URL í™˜ê²½ë³€ìˆ˜ (ë˜ëŠ” ì§ì ‘ ì…ë ¥)
        webhook = os.environ.get("APPS_SCRIPT_WEBHOOK")
        
        if final_data and webhook:
            chunk_size = 4000
            for i in range(0, len(final_data), chunk_size):
                chunk = final_data[i:i+chunk_size]
                try:
                    requests.post(webhook, json=chunk)
                    print(f"  -> Chunk {i//chunk_size + 1} sent.")
                    time.sleep(1)
                except Exception as e:
                    print(f"âŒ Send Error: {e}")
            print("âœ¨ All Scrapers Completed Successfully!")
        else:
            print("âš ï¸ No webhook URL found or empty data. Check 'APPS_SCRIPT_WEBHOOK' env var.")

    except Exception as main_e:
        print("ğŸ”¥ FATAL ERROR: Script crashed.")
        print(traceback.format_exc())
