import os
import re
import time
import json
import requests
from datetime import datetime
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.common.by import By

# ================= ì„¤ì • =================
YOUTUBE_API_KEY = "AIzaSyDFFZNYygA85qp5p99qUG2Mh8Kl5qoLip4"

TARGET_URLS = {
    # 1. Trending (API ìœ ì§€)
    "KR_Daily_Trending": "https://charts.youtube.com/charts/TrendingVideos/kr/RightNow",
    "US_Daily_Trending": "https://charts.youtube.com/charts/TrendingVideos/us/RightNow",
    
    # 2. Daily MV (Hidden Div íƒ€ê²©)
    "KR_Daily_Top_MV": "https://charts.youtube.com/charts/TopVideos/kr/daily",
    "US_Daily_Top_MV": "https://charts.youtube.com/charts/TopVideos/us/daily",

    # 3. Weekly (Visible Metric íƒ€ê²©)
    "KR_Weekly_Top_MV": "https://charts.youtube.com/charts/TopVideos/kr/weekly",
    "US_Weekly_Top_MV": "https://charts.youtube.com/charts/TopVideos/us/weekly",
    "KR_Weekly_Top_Songs": "https://charts.youtube.com/charts/TopSongs/kr/weekly",
    "US_Weekly_Top_Songs": "https://charts.youtube.com/charts/TopSongs/us/weekly",
    
    # 4. Shorts (Shadow DOM ID ì¶”ì¶œ)
    "KR_Daily_Top_Shorts": "https://charts.youtube.com/charts/TopShortsSongs/kr/daily",
    "US_Daily_Top_Shorts": "https://charts.youtube.com/charts/TopShortsSongs/us/daily"
}

# ================= ìœ í‹¸ë¦¬í‹° =================
def parse_count_strict(text):
    if not text: return 0
    t = str(text).lower().strip().replace(',', '')
    
    multiplier = 1
    if 'k' in t: multiplier = 1_000
    elif 'm' in t: multiplier = 1_000_000
    elif 'b' in t: multiplier = 1_000_000_000
    
    clean = re.sub(r'[^\d.]', '', t)
    if not clean: return 0
    
    try:
        val = float(clean)
        return int(val * multiplier)
    except: return 0

def get_driver():
    chrome_options = Options()
    chrome_options.add_argument("--headless") 
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--window-size=1920,1080")
    chrome_options.add_argument("--lang=en-US") 
    service = Service(ChromeDriverManager().install())
    return webdriver.Chrome(service=service, options=chrome_options)

# ================= [í•µì‹¬] Shorts ID ì¶”ì¶œ (Shadow DOM ì „ìš©) =================
def extract_shorts_ids_from_shadow(driver):
    # 1. ëê¹Œì§€ ìŠ¤í¬ë¡¤ (Lazy Loading í•´ê²° - 30ë²ˆ ë°˜ë³µ)
    # ë‹˜ì´ ì“°ë˜ ì½”ë“œëŠ” ìŠ¤í¬ë¡¤ 1ë²ˆì´ë¼ ë°ì´í„° ë‹¤ ëª» ê°€ì ¸ì˜´. ì´ê±´ ëê¹Œì§€ ë‚´ë¦¼.
    last_height = driver.execute_script("return document.body.scrollHeight")
    for _ in range(30):
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(0.5)
        new_height = driver.execute_script("return document.body.scrollHeight")
        if new_height == last_height:
            break
        last_height = new_height
    time.sleep(2)

    # 2. Shadow DOM ëš«ê³  ID ì¶”ì¶œ (ì´ê²Œ ë‹˜ ì½”ë“œì—” ì—†ìŒ)
    video_ids = []
    try:
        # CSS Selectorë¡œ Shadow DOM ë‚´ë¶€ì˜ ë§í¬ë¥¼ ì§ì ‘ íƒ€ê²©
        elements = driver.find_elements(By.CSS_SELECTOR, "a#video-title-link")
        for el in elements:
            href = el.get_attribute("href")
            if href:
                m = re.search(r"v=([a-zA-Z0-9_-]{11})", href)
                if m:
                    video_ids.append(m.group(1))
    except Exception as e:
        print(f"Shadow DOM extraction error: {e}")
        
    return video_ids

# ================= Shorts ë”¥ë‹¤ì´ë¸Œ (Selenium ë°©ì‹ - ì•ˆì „í•¨) =================
def get_shorts_count_deep(driver, video_id):
    if not video_id: return 0
    url = f"https://www.youtube.com/source/{video_id}/shorts"
    try:
        driver.get(url)
        time.sleep(1.5)
        
        # ì „ì²´ ì†ŒìŠ¤ì—ì„œ Regex íƒìƒ‰ (ê°€ì¥ ì•ˆì „)
        body_text = driver.page_source
        match = re.search(r'([\d,.]+[KMB]?)\s*(shorts|videos)', body_text, re.IGNORECASE)
        if match:
            return parse_count_strict(match.group(1))
            
        # ë°±ì—…
        body_text_simple = driver.find_element(By.TAG_NAME, "body").text
        match2 = re.search(r'([\d,.]+[KMB]?)\s*(shorts|videos)', body_text_simple, re.IGNORECASE)
        if match2:
            return parse_count_strict(match2.group(1))
            
        return 0
    except: return 0

# ================= API ì¡°íšŒ (Trending) =================
def get_views_from_api(video_ids):
    if not video_ids: return {}
    url = "https://www.googleapis.com/youtube/v3/videos"
    stats_map = {}
    for i in range(0, len(video_ids), 50):
        chunk = video_ids[i:i+50]
        params = {"part": "statistics", "id": ",".join(chunk), "key": YOUTUBE_API_KEY}
        try:
            res = requests.get(url, params=params).json()
            if "items" in res:
                for item in res["items"]:
                    vid = item["id"]
                    view_count = int(item["statistics"].get("viewCount", 0))
                    stats_map[vid] = view_count
        except: pass
    return stats_map

# ================= ë©”ì¸ ìŠ¤í¬ë˜í¼ =================
def scrape_chart(chart_name, url, driver):
    print(f"ğŸš€ Scraping {chart_name}...")
    driver.get(url)
    time.sleep(5)
    
    data_list = []
    today = datetime.now().strftime("%Y-%m-%d")
    
    is_trending = "Trending" in chart_name
    is_shorts = "Shorts" in chart_name
    is_daily_mv = "Daily_Top_MV" in chart_name
    is_weekly = "Weekly" in chart_name
    
    # ---------------------------------------------------------
    # CASE 1: Shorts (Shadow DOM ì²˜ë¦¬ - ë‹˜ ì½”ë“œë‘ ë‹¤ë¥¸ ë¶€ë¶„)
    # ---------------------------------------------------------
    if is_shorts:
        print("  â†³ Shorts Mode: Extracting IDs from Shadow DOM...")
        video_ids = extract_shorts_ids_from_shadow(driver) # ì—¬ê¸°ì„œ ID 100ê°œë¥¼ ë¨¼ì € ë‹¤ ê°€ì ¸ì˜´
        
        # BS4ë¡œ ê»ë°ê¸°(ì œëª©/ì•„í‹°ìŠ¤íŠ¸)ë§Œ íŒŒì‹±
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        rows = soup.find_all('ytmc-entry-row')
        
        for idx, row in enumerate(rows):
            try:
                title = row.find('div', class_='title').get_text(strip=True)
                artist_tag = row.find('span', class_='artistName') or row.find('div', class_='subtitle')
                artist = artist_tag.get_text(strip=True) if artist_tag else ""
                
                # ID ë§¤í•‘ (ìˆœì„œ ê¸°ë°˜)
                vid = video_ids[idx] if idx < len(video_ids) else ""
                
                # Shorts ì¡°íšŒìˆ˜ëŠ” ë‚˜ì¤‘ì— ë”¥ë‹¤ì´ë¸Œë¡œ ì±„ì›€ (ì¼ë‹¨ 0)
                data_list.append({
                    "Date": today, "Chart": chart_name, "Rank": idx+1,
                    "Title": title, "Artist": artist, "Video_ID": vid, "Views": 0
                })
            except: continue

    # ---------------------------------------------------------
    # CASE 2: MV / Songs / Trending (ê¸°ì¡´ ë°©ì‹ ìœ ì§€)
    # ---------------------------------------------------------
    else:
        # ê¸°ë³¸ ìŠ¤í¬ë¡¤
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(3)
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        rows = soup.find_all('ytmc-entry-row')
        
        for idx, row in enumerate(rows):
            try:
                title = row.find('div', class_='title').get_text(strip=True)
                artist_tag = row.find('span', class_='artistName') or row.find('div', class_='subtitle')
                artist = artist_tag.get_text(strip=True) if artist_tag else ""
                
                # ID ì¶”ì¶œ (Daily MV ë“± ì¼ë°˜ ì°¨íŠ¸ìš©)
                vid = ""
                anchor = row.find('a')
                if anchor and 'href' in anchor.attrs:
                    m = re.search(r"v=([A-Za-z0-9_-]{11})", anchor['href'])
                    if m: vid = m.group(1)
                
                if not vid: # ë°±ì—…
                    img = row.find('img')
                    if img and 'src' in img.attrs:
                        m = re.search(r'/vi(?:_webp)?/([a-zA-Z0-9_-]{11})', img['src'])
                        if m: vid = m.group(1)

                final_views = 0
                
                # [ë·° ì¹´ìš´íŠ¸ ë¡œì§]
                if is_trending:
                    pass # APIë¡œ ì±„ì›€
                
                elif is_daily_mv:
                    # tablet-non-displayed-metric ì¤‘ ê°€ì¥ í° ê°’ ì°¾ê¸°
                    hidden_divs = row.find_all('div', class_='tablet-non-displayed-metric')
                    max_val = 0
                    for h in hidden_divs:
                        val = parse_count_strict(h.get_text(strip=True))
                        if val > max_val: max_val = val
                    final_views = max_val
                    
                elif is_weekly:
                    # í™”ë©´ ë§¨ ë metric
                    metrics = row.find_all('div', class_='metric')
                    if metrics:
                        final_views = parse_count_strict(metrics[-1].get_text(strip=True))
                
                data_list.append({
                    "Date": today, "Chart": chart_name, "Rank": idx+1,
                    "Title": title, "Artist": artist, "Video_ID": vid, "Views": final_views
                })
            except: continue

    return data_list

# ================= ë©”ì¸ ì‹¤í–‰ =================
if __name__ == "__main__":
    driver = get_driver()
    final_data = []
    
    for name, url in TARGET_URLS.items():
        try:
            # 1. ìˆ˜ì§‘
            chart_data = scrape_chart(name, url, driver)
            
            # 2. í›„ì²˜ë¦¬
            if "Trending" in name:
                ids = [d["Video_ID"] for d in chart_data if d["Video_ID"]]
                if ids:
                    api_stats = get_views_from_api(ids)
                    for item in chart_data:
                        if item["Video_ID"] in api_stats:
                            item["Views"] = api_stats[item["Video_ID"]]
                            
            elif "Shorts" in name:
                print(f"  â†³ Deep diving {len(chart_data)} shorts...")
                for item in chart_data:
                    if item["Video_ID"]:
                        cnt = get_shorts_count_deep(driver, item["Video_ID"])
                        item["Views"] = cnt
            
            final_data.extend(chart_data)
            print(f"âœ… {name}: {len(chart_data)} rows done.")
            
        except Exception as e:
            print(f"Error on {name}: {e}")

    driver.quit()
    
    # ì „ì†¡
    webhook = os.environ.get("APPS_SCRIPT_WEBHOOK")
    if final_data and webhook:
        print(f"Total {len(final_data)} rows. Sending...")
        try:
            requests.post(webhook, json=final_data)
            print("Success!")
        except Exception as e:
            print(f"Send Error: {e}")
    else:
        print("No webhook or data.")
